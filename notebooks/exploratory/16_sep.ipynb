{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, log_loss, roc_auc_score,\n",
    "    roc_curve, brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Import custom models\n",
    "# Removed old path hack; using package imports))\n",
    "from llm_prior_project.priors.target_informed_model import TargetInformedModel\n",
    "from llm_prior_project.priors.target_elicitor import LLMTargetElicitor\n",
    "from llm_prior_project.models.target_model import SklearnTargetModel\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hungarian: (270, 6) Cleveland: (303, 6)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# loading the data \n",
    "def load_heart_dataset(path, features, outcome=\"num\"):\n",
    "    columns = [\n",
    "        \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "        \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"\n",
    "    ]\n",
    "    df = pd.read_csv(path, header=None, names=columns, na_values=\"?\")\n",
    "    df[outcome] = (df[outcome] > 0).astype(int)\n",
    "    df = df[features + [outcome]].dropna()\n",
    "    return df[features], df[outcome]\n",
    "\n",
    "features = [\"age\", \"sex\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "\n",
    "X, y = load_heart_dataset(\"data/heart+disease/processed.hungarian.data\", features)\n",
    "X_cleveland, y_cleveland = load_heart_dataset(\"data/heart+disease/processed.cleveland.data\", features)\n",
    "\n",
    "print(\"Hungarian:\", X.shape, \"Cleveland:\", X_cleveland.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using user-crafted prompt\n",
      "Feature names expected: ['age', 'sex', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
      "Prompt length: 2934 characters\n",
      "Prompt validation: PASSED\n",
      "Calling LLM API...\n",
      "Got LLM response (2630 characters)\n",
      "Response preview: 1. **Leverage Knowledge & Simulate Literature Use**  \n",
      "   The Framingham Heart Study, MONICA project, and various meta-analyses have provided valuable insights into the risk factors for coronary artery...\n",
      "Parsing LLM response...\n",
      "Successfully extracted targets: [0.05, 0.5, 0.02, 0.002, -0.01, 0.4]\n",
      "Extracted targets: [0.05, 0.5, 0.02, 0.002, -0.01, 0.4]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "custom_prompt = custom_prompt = \"\"\"\n",
    "\n",
    "You are an expert in **biostatistics and cardiovascular epidemiology**.  \n",
    "For the **target-informed logistic regression model** provided below, which predicts coronary artery disease (CAD), your task is to propose and justify suitable target values for the regression coefficients.  \n",
    "\n",
    "Unlike standard ridge regression, which shrinks coefficients toward **zero**, this method shrinks coefficients toward **pre-specified targets** $\\mu = (\\mu_1, \\ldots, \\mu_p)$, based on domain knowledge.  \n",
    "\n",
    "---\n",
    "\n",
    "**Model Details:**  \n",
    "- Response: $y \\in \\{0,1\\}$ (0 = healthy, 1 = CAD)  \n",
    "- Linear Predictor:  \n",
    "\n",
    "\\[\n",
    "\\eta = \\beta_0 + \\beta_1 \\cdot \\text{age} + \\beta_2 \\cdot \\text{sex} + \\beta_3 \\cdot \\text{trestbps} + \\beta_4 \\cdot \\text{chol} + \\beta_5 \\cdot \\text{thalach} + \\beta_6 \\cdot \\text{oldpeak}\n",
    "\\]\n",
    "\n",
    "- Prediction:  \n",
    "\n",
    "\\[\n",
    "P(y=1|X) = \\frac{1}{1 + \\exp(-\\eta)}\n",
    "\\]\n",
    "\n",
    "- Objective Function (Target-Informed Ridge Penalty):  \n",
    "\n",
    "\\[\n",
    "\\min_\\beta \\; - \\log L(\\beta \\,|\\, X,y) + \\alpha \\sum_{j=1}^p (\\beta_j - \\mu_j)^2\n",
    "\\]\n",
    "\n",
    "where $\\mu_j$ are the **target values** you will propose.  \n",
    "\n",
    "---\n",
    "\n",
    "**Predictor Details:**  \n",
    "- `age`: in years  \n",
    "- `sex`: categorical (1 = male; 0 = female)  \n",
    "- `trestbps`: resting blood pressure (mm Hg on admission)  \n",
    "- `chol`: serum cholesterol (mg/dl)  \n",
    "- `thalach`: maximum heart rate achieved (bpm)  \n",
    "- `oldpeak`: ST depression induced by exercise relative to rest (mm)  \n",
    "\n",
    "---\n",
    "\n",
    "**Your Response Should:**  \n",
    "\n",
    "1. **Leverage Knowledge & Simulate Literature Use**  \n",
    "   - Briefly state how you use your epidemiological knowledge (e.g., Framingham Study, MONICA project, meta-analyses) to inform coefficient expectations.  \n",
    "   - Do *not* rely on the Cleveland Heart Disease dataset, but instead general domain knowledge.  \n",
    "\n",
    "2. **Propose Target Coefficients ($\\mu_j$)**  \n",
    "   - Provide target values $\\mu_j$ for each predictor, representing the expected effect on the log-odds of CAD.  \n",
    "   - Justify the direction (positive/negative) and plausible magnitude of each coefficient in natural language.  \n",
    "\n",
    "3. **Rationale for Each Target**  \n",
    "   - For each predictor, explain why the coefficient should be positive/negative, and roughly how large, based on prior evidence.  \n",
    "   - For $\\beta_0$, clarify its interpretation (baseline log-odds when all predictors are zero) and how you approximate it.  \n",
    "\n",
    "4. **Uncertainty & Strength of Belief**  \n",
    "   - Discuss how confident you are about each target. Which ones are well-established (e.g., age, sex), and which are more uncertain (e.g., cholesterol in small hospital samples)?  \n",
    "   - If useful, you may also describe an alternative \u201cweaker\u201d set of targets closer to zero, to represent more cautious shrinkage.  \n",
    "\n",
    "---\n",
    "\n",
    "**Output Format:**  \n",
    "After providing your reasoning in detail, end your answer with a JSON object summarizing the chosen target values only:  \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"targets\": {\n",
    "    \"age\": ... ,\n",
    "    \"sex\": ... ,\n",
    "    \"trestbps\": ... ,\n",
    "    \"chol\": ... ,\n",
    "    \"thalach\": ... ,\n",
    "    \"oldpeak\": ...\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "elicitor = LLMTargetElicitor(model_name=\"gpt-4\")\n",
    "result = elicitor.get_targets_with_prompt(custom_prompt, features)\n",
    "\n",
    "if result:\n",
    "    targets = result[\"targets\"]\n",
    "    print(\"Extracted targets:\", targets)\n",
    "else:\n",
    "    targets = [0.0] * len(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05, 0.5, 0.02, 0.002, -0.01, 0.4]\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def evaluate_models(model, baseline, X, y):\n",
    "    \"\"\"\n",
    "    Evaluate baseline and informed models on given data.\n",
    "    Works with either numpy arrays or pandas DataFrames.\n",
    "    \"\"\"\n",
    "    # Ensure numpy arrays\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    # Baseline (sklearn logistic regression)\n",
    "    baseline_probs = baseline.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Informed model already returns probabilities\n",
    "    informed_probs = model.predict(X)\n",
    "\n",
    "    return {\n",
    "        \"baseline_accuracy\": accuracy_score(y, (baseline_probs > 0.5).astype(int)),\n",
    "        \"baseline_log_loss\": log_loss(y, baseline_probs),\n",
    "        \"baseline_auc\": roc_auc_score(y, baseline_probs),\n",
    "        \"informed_accuracy\": accuracy_score(y, (informed_probs > 0.5).astype(int)),\n",
    "        \"informed_log_loss\": log_loss(y, informed_probs),\n",
    "        \"informed_auc\": roc_auc_score(y, informed_probs)\n",
    "    }\n",
    "\n",
    "\n",
    "def cross_val_evaluate(X, y, feature_names, targets, alpha=1.0, n_splits=5):\n",
    "    \"\"\"\n",
    "    Compare baseline vs target-informed logistic regression using cross-validation.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    \n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Baseline logistic regression\n",
    "        baseline = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "        \n",
    "        # Target-informed logistic regression\n",
    "        informed = TargetInformedModel(alpha=alpha, model_type=\"logistic\").fit(\n",
    "            X_train, y_train, feature_names=feature_names, targets=targets\n",
    "        )\n",
    "        \n",
    "        results.append(evaluate_models(informed, baseline, X_test, y_test))\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def cross_val_grid_alphas(X, y, feature_names, targets, alphas, n_splits=5):\n",
    "    \"\"\"\n",
    "    Run cross-validation across multiple alpha values for TargetInformedModel.\n",
    "    Returns per-fold results, a summary table, and the best alpha.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    for alpha in alphas:\n",
    "        cv_results = cross_val_evaluate(\n",
    "            X, y, feature_names, targets, alpha=alpha, n_splits=n_splits\n",
    "        )\n",
    "        cv_results[\"alpha\"] = alpha\n",
    "        all_results.append(cv_results)\n",
    "\n",
    "    results = pd.concat(all_results, ignore_index=True)\n",
    "    summary = results.groupby(\"alpha\").mean()\n",
    "\n",
    "    # Select alpha with lowest informed log_loss\n",
    "    best_alpha = summary[\"informed_log_loss\"].idxmin()\n",
    "    print(\"\\nCross-validation summary:\")\n",
    "    print(summary)\n",
    "    print(\"\\nSelected alpha (by log_loss):\", best_alpha)\n",
    "\n",
    "    return results, summary, best_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation summary:\n",
      "       baseline_accuracy  baseline_log_loss  baseline_auc  informed_accuracy  \\\n",
      "alpha                                                                          \n",
      "0.01            0.814815           0.450018      0.849259           0.811111   \n",
      "0.10            0.814815           0.450018      0.849259           0.811111   \n",
      "0.50            0.814815           0.450018      0.849259           0.811111   \n",
      "1.00            0.814815           0.450018      0.849259           0.814815   \n",
      "2.00            0.814815           0.450018      0.849259           0.814815   \n",
      "5.00            0.814815           0.450018      0.849259           0.825926   \n",
      "10.00           0.814815           0.450018      0.849259           0.822222   \n",
      "\n",
      "       informed_log_loss  informed_auc  \n",
      "alpha                                   \n",
      "0.01            0.452301      0.848666  \n",
      "0.10            0.451825      0.849548  \n",
      "0.50            0.450621      0.851307  \n",
      "1.00            0.449237      0.850719  \n",
      "2.00            0.448934      0.851002  \n",
      "5.00            0.450193      0.846629  \n",
      "10.00           0.454907      0.847517  \n",
      "\n",
      "Selected alpha (by log_loss): 2.0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "alphas = [0.01, 0.1, 0.5, 1, 2, 5, 10]\n",
    "grid_results, summary, best_alpha = cross_val_grid_alphas(\n",
    "    X.values, y.values, features, targets, alphas, n_splits=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hungarian dataset comparison:\n",
      "          accuracy  log_loss       auc\n",
      "logistic  0.818519  0.424243  0.862441\n",
      "ridge     0.825926  0.425129  0.861269\n",
      "informed  0.674074  0.602512  0.860214\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --- Fit models ---\n",
    "# 1. Standard logistic regression (no penalty)\n",
    "log_reg = LogisticRegression(\n",
    "    penalty=None, max_iter=1000, solver=\"lbfgs\"\n",
    ").fit(X, y)\n",
    "\n",
    "# 2. Logistic regression with L2 ridge penalty\n",
    "ridge_reg = LogisticRegression(\n",
    "    penalty=\"l2\", C=1.0, max_iter=1000, solver=\"lbfgs\"\n",
    ").fit(X, y)\n",
    "\n",
    "# 3. Target-informed logistic regression (new sklearn hack model)\n",
    "informed_reg = SklearnTargetModel(\n",
    "    alpha=best_alpha, model_type=\"logistic\", targets=targets\n",
    ").fit(X.values, y.values, feature_names=features)\n",
    "\n",
    "# --- Evaluate ---\n",
    "def evaluate_three_models(X, y, log_reg, ridge_reg, informed_reg):\n",
    "    results = {}\n",
    "\n",
    "    # Standard logistic regression\n",
    "    probs_log = log_reg.predict_proba(X)[:, 1]\n",
    "    results[\"logistic\"] = {\n",
    "        \"accuracy\": accuracy_score(y, (probs_log > 0.5)),\n",
    "        \"log_loss\": log_loss(y, probs_log),\n",
    "        \"auc\": roc_auc_score(y, probs_log)\n",
    "    }\n",
    "\n",
    "    # Ridge logistic regression\n",
    "    probs_ridge = ridge_reg.predict_proba(X)[:, 1]\n",
    "    results[\"ridge\"] = {\n",
    "        \"accuracy\": accuracy_score(y, (probs_ridge > 0.5)),\n",
    "        \"log_loss\": log_loss(y, probs_ridge),\n",
    "        \"auc\": roc_auc_score(y, probs_ridge)\n",
    "    }\n",
    "\n",
    "    # Target-informed logistic regression\n",
    "    probs_inf = informed_reg.predict(X.values)\n",
    "    results[\"informed\"] = {\n",
    "        \"accuracy\": accuracy_score(y, (probs_inf > 0.5)),\n",
    "        \"log_loss\": log_loss(y, probs_inf),\n",
    "        \"auc\": roc_auc_score(y, probs_inf)\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# Run evaluation on Hungarian dataset\n",
    "metrics_three = evaluate_three_models(X, y, log_reg, ridge_reg, informed_reg)\n",
    "print(\"Hungarian dataset comparison:\")\n",
    "print(metrics_three)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def cross_val_search_targets(X, y, features, target_candidates, alphas, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Cross-validate to find best (targets, alpha) combination for Target-Informed Logistic Regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        Feature matrix\n",
    "    y : array-like\n",
    "        Binary target vector\n",
    "    features : list of str\n",
    "        Feature names\n",
    "    target_candidates : list of np.array\n",
    "        List of candidate target vectors (\u03bc) to try\n",
    "    alphas : list of float\n",
    "        Regularisation strengths\n",
    "    n_splits : int\n",
    "        Number of CV folds\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : pd.DataFrame\n",
    "        Per-fold metrics for all (targets, alpha) combos\n",
    "    summary : pd.DataFrame\n",
    "        Mean metrics per (targets, alpha) combo\n",
    "    best_combo : dict\n",
    "        Dictionary with best targets, alpha, and their metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    all_results = []\n",
    "\n",
    "    for mu in target_candidates:\n",
    "        for alpha in alphas:\n",
    "            fold_metrics = []\n",
    "            for train_idx, test_idx in skf.split(X, y):\n",
    "                X_train, X_test = X[train_idx], X[test_idx]\n",
    "                y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "                model = SklearnTargetModel(alpha=alpha, model_type=\"logistic\", targets=mu).fit(\n",
    "                    X_train, y_train, feature_names=features\n",
    "                )\n",
    "                probs = model.predict(X_test)\n",
    "\n",
    "                fold_metrics.append({\n",
    "                    \"alpha\": alpha,\n",
    "                    \"targets\": tuple(mu.round(3)),  # store as tuple for readability\n",
    "                    \"log_loss\": log_loss(y_test, probs),\n",
    "                    \"accuracy\": accuracy_score(y_test, (probs > 0.5)),\n",
    "                    \"auc\": roc_auc_score(y_test, probs),\n",
    "                })\n",
    "            all_results.extend(fold_metrics)\n",
    "\n",
    "    results = pd.DataFrame(all_results)\n",
    "    summary = results.groupby([\"alpha\", \"targets\"]).mean().reset_index()\n",
    "\n",
    "    # Best by log_loss\n",
    "    best_idx = summary[\"log_loss\"].idxmin()\n",
    "    best_combo = summary.iloc[best_idx].to_dict()\n",
    "\n",
    "    print(\"Best combination found:\")\n",
    "    print(best_combo)\n",
    "\n",
    "    return results, summary, best_combo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combination found:\n",
      "{'alpha': 1.0, 'targets': (np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)), 'log_loss': 0.4500173516917004, 'accuracy': 0.8148148148148149, 'auc': 0.849259400729989}\n",
      "\n",
      "CV summary:\n",
      "    alpha                         targets  log_loss  accuracy       auc\n",
      "10   1.00  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  0.450017  0.814815  0.849259\n",
      "13   2.00  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  0.450462  0.825926  0.848671\n",
      "7    0.50  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  0.450688  0.814815  0.851019\n",
      "4    0.10  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  0.452181  0.807407  0.849548\n",
      "1    0.01  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  0.452713  0.807407  0.848666\n"
     ]
    }
   ],
   "source": [
    "# Candidate target sets\n",
    "target_candidates = [\n",
    "    np.zeros(len(features)),             # shrink to zero\n",
    "    np.array([0.3, 0.5, 0.1, 0.05, -0.3, 0.7]),  # literature-based example\n",
    "    LogisticRegression(penalty=None, max_iter=1000).fit(X, y).coef_[0]  # MLE from data\n",
    "]\n",
    "\n",
    "alphas = [0.01, 0.1, 0.5, 1, 2, 5]\n",
    "\n",
    "cv_results, cv_summary, best = cross_val_search_targets(\n",
    "    X.values, y.values, features, target_candidates, alphas, n_splits=5\n",
    ")\n",
    "\n",
    "print(\"\\nCV summary:\")\n",
    "print(cv_summary.sort_values(\"log_loss\").head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_search_and_generalize(\n",
    "    X_train, y_train, X_test, y_test, features,\n",
    "    target_candidates, alphas, n_splits=5, random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Cross-validate to find best (targets, alpha) on training set.\n",
    "    2. Refit best model on full training set.\n",
    "    3. Evaluate on external test set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : pd.DataFrame\n",
    "        Per-fold metrics for all (targets, alpha) combos.\n",
    "    summary : pd.DataFrame\n",
    "        Mean CV metrics per (targets, alpha).\n",
    "    best_combo : dict\n",
    "        Best combo from CV (alpha, targets).\n",
    "    external_metrics : dict\n",
    "        Evaluation metrics on external test set.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    all_results = []\n",
    "\n",
    "    for mu in target_candidates:\n",
    "        for alpha in alphas:\n",
    "            fold_metrics = []\n",
    "            for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "                X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "                y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "                model = SklearnTargetModel(alpha=alpha, model_type=\"logistic\", targets=mu).fit(\n",
    "                    X_tr, y_tr, feature_names=features\n",
    "                )\n",
    "                probs = model.predict(X_val)\n",
    "\n",
    "                fold_metrics.append({\n",
    "                    \"alpha\": alpha,\n",
    "                    \"targets\": tuple(mu.round(3)),\n",
    "                    \"log_loss\": log_loss(y_val, probs),\n",
    "                    \"accuracy\": accuracy_score(y_val, (probs > 0.5)),\n",
    "                    \"auc\": roc_auc_score(y_val, probs),\n",
    "                })\n",
    "            all_results.extend(fold_metrics)\n",
    "\n",
    "    results = pd.DataFrame(all_results)\n",
    "    summary = results.groupby([\"alpha\", \"targets\"]).mean().reset_index()\n",
    "\n",
    "    # Best by log_loss\n",
    "    best_idx = summary[\"log_loss\"].idxmin()\n",
    "    best_combo = summary.iloc[best_idx].to_dict()\n",
    "\n",
    "    print(\"Best combination found (CV):\")\n",
    "    print(best_combo)\n",
    "\n",
    "    # --- Refit best model on full training set ---\n",
    "    best_alpha = best_combo[\"alpha\"]\n",
    "    best_targets = np.array(best_combo[\"targets\"], dtype=float)\n",
    "\n",
    "    final_model = SklearnTargetModel(alpha=best_alpha, model_type=\"logistic\", targets=best_targets).fit(\n",
    "        X_train, y_train, feature_names=features\n",
    "    )\n",
    "\n",
    "    # --- External test evaluation ---\n",
    "    probs_test = final_model.predict(X_test)\n",
    "    external_metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, (probs_test > 0.5)),\n",
    "        \"log_loss\": log_loss(y_test, probs_test),\n",
    "        \"auc\": roc_auc_score(y_test, probs_test),\n",
    "    }\n",
    "\n",
    "    print(\"\\nExternal test set evaluation (Cleveland):\")\n",
    "    print(external_metrics)\n",
    "\n",
    "    return results, summary, best_combo, external_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combination found (CV):\n",
      "{'alpha': 1.0, 'targets': (np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)), 'log_loss': 0.4500173516917004, 'accuracy': 0.8148148148148149, 'auc': 0.849259400729989}\n",
      "\n",
      "External test set evaluation (Cleveland):\n",
      "{'accuracy': 0.7491749174917491, 'log_loss': 0.5577224833451117, 'auc': 0.8054921916125636}\n",
      "\n",
      "Top CV combos:\n",
      "    alpha                         targets  log_loss  accuracy       auc\n",
      "10   1.00  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  0.450017  0.814815  0.849259\n",
      "13   2.00  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  0.450462  0.825926  0.848671\n",
      "7    0.50  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  0.450688  0.814815  0.851019\n",
      "4    0.10  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  0.452181  0.807407  0.849548\n",
      "1    0.01  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  0.452713  0.807407  0.848666\n"
     ]
    }
   ],
   "source": [
    "target_candidates = [\n",
    "    np.zeros(len(features)),  # zero target (ridge)\n",
    "    LogisticRegression(penalty=None, max_iter=1000).fit(X, y).coef_[0],  # MLE targets\n",
    "    np.array([0.3, 0.5, 0.1, 0.05, -0.3, 0.7])  # example domain-informed\n",
    "]\n",
    "\n",
    "alphas = [0.01, 0.1, 0.5, 1, 2, 5]\n",
    "\n",
    "cv_results, cv_summary, best, external_metrics = cross_val_search_and_generalize(\n",
    "    X.values, y.values,\n",
    "    X_cleveland.values, y_cleveland.values,\n",
    "    features, target_candidates, alphas,\n",
    "    n_splits=5\n",
    ")\n",
    "\n",
    "print(\"\\nTop CV combos:\")\n",
    "print(cv_summary.sort_values(\"log_loss\").head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def low_data_experiment(\n",
    "    X_train, y_train, X_test, y_test, features,\n",
    "    targets_prior, alphas=[1.0], fractions=[0.1, 0.2, 0.3], random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Test generalisation with small subsets of training data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train : Hungarian data\n",
    "    X_test, y_test   : Cleveland data\n",
    "    targets_prior    : np.array\n",
    "        Expert-informed targets for informed model\n",
    "    alphas : list\n",
    "        Regularisation strengths to try for ridge and informed models\n",
    "    fractions : list\n",
    "        Fractions of training data to subsample\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for frac in fractions:\n",
    "        # Subsample Hungarian data\n",
    "        X_sub, y_sub = resample(\n",
    "            X_train, y_train,\n",
    "            replace=False,\n",
    "            n_samples=int(frac * len(y_train)),\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # --- Standard logistic regression ---\n",
    "        log_reg = LogisticRegression(\n",
    "            penalty=None, max_iter=1000, solver=\"lbfgs\"\n",
    "        ).fit(X_sub, y_sub)\n",
    "        probs_log = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        results.append({\n",
    "            \"train_frac\": frac,\n",
    "            \"model\": \"logistic\",\n",
    "            \"accuracy\": accuracy_score(y_test, (probs_log > 0.5)),\n",
    "            \"log_loss\": log_loss(y_test, probs_log),\n",
    "            \"auc\": roc_auc_score(y_test, probs_log),\n",
    "        })\n",
    "\n",
    "        # --- Ridge logistic regression ---\n",
    "        for alpha in alphas:\n",
    "            ridge_reg = LogisticRegression(\n",
    "                penalty=\"l2\", C=1.0/alpha, max_iter=1000, solver=\"lbfgs\"\n",
    "            ).fit(X_sub, y_sub)\n",
    "            probs_ridge = ridge_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            results.append({\n",
    "                \"train_frac\": frac,\n",
    "                \"model\": f\"ridge (alpha={alpha})\",\n",
    "                \"accuracy\": accuracy_score(y_test, (probs_ridge > 0.5)),\n",
    "                \"log_loss\": log_loss(y_test, probs_ridge),\n",
    "                \"auc\": roc_auc_score(y_test, probs_ridge),\n",
    "            })\n",
    "\n",
    "        # --- Target-informed logistic regression ---\n",
    "        for alpha in alphas:\n",
    "            informed_reg = SklearnTargetModel(\n",
    "                alpha=alpha, model_type=\"logistic\", targets=targets_prior\n",
    "            ).fit(X_sub, y_sub, feature_names=features)\n",
    "            probs_inf = informed_reg.predict(X_test)\n",
    "\n",
    "            results.append({\n",
    "                \"train_frac\": frac,\n",
    "                \"model\": f\"informed (alpha={alpha})\",\n",
    "                \"accuracy\": accuracy_score(y_test, (probs_inf > 0.5)),\n",
    "                \"log_loss\": log_loss(y_test, probs_inf),\n",
    "                \"auc\": roc_auc_score(y_test, probs_inf),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train_frac                 model  accuracy  log_loss       auc\n",
      "4          0.1  informed (alpha=0.5)  0.567657  4.147692  0.797157\n",
      "5          0.1  informed (alpha=1.0)  0.554455  5.914845  0.787024\n",
      "6          0.1  informed (alpha=2.0)  0.547855  7.621389  0.779654\n",
      "0          0.1              logistic  0.739274  0.570479  0.824574\n",
      "1          0.1     ridge (alpha=0.5)  0.716172  0.584779  0.800579\n",
      "2          0.1     ridge (alpha=1.0)  0.699670  0.607948  0.785533\n",
      "3          0.1     ridge (alpha=2.0)  0.689769  0.638301  0.769082\n",
      "11         0.2  informed (alpha=0.5)  0.458746  1.991015  0.789700\n",
      "12         0.2  informed (alpha=1.0)  0.742574  0.772251  0.820319\n",
      "13         0.2  informed (alpha=2.0)  0.600660  2.715390  0.806019\n",
      "7          0.2              logistic  0.745875  0.688388  0.826505\n",
      "8          0.2     ridge (alpha=0.5)  0.745875  0.587038  0.820758\n",
      "9          0.2     ridge (alpha=1.0)  0.752475  0.575426  0.817512\n",
      "10         0.2     ridge (alpha=2.0)  0.739274  0.573787  0.815143\n",
      "18         0.3  informed (alpha=0.5)  0.458746  1.357774  0.790095\n",
      "19         0.3  informed (alpha=1.0)  0.735974  0.607009  0.813652\n",
      "20         0.3  informed (alpha=2.0)  0.650165  1.624717  0.812774\n",
      "14         0.3              logistic  0.745875  0.631659  0.811458\n",
      "15         0.3     ridge (alpha=0.5)  0.735974  0.591411  0.810844\n",
      "16         0.3     ridge (alpha=1.0)  0.729373  0.571608  0.809923\n",
      "17         0.3     ridge (alpha=2.0)  0.729373  0.551595  0.808826\n"
     ]
    }
   ],
   "source": [
    "# Example prior (make this from domain knowledge!)\n",
    "expert_targets = np.array([0.03, 0.5, 0.1, 0.05, -0.4, 0.7])\n",
    "\n",
    "fractions = [0.1, 0.2, 0.3]\n",
    "alphas = [0.5, 1.0, 2.0]\n",
    "\n",
    "low_data_results = low_data_experiment(\n",
    "    X.values, y.values,\n",
    "    X_cleveland.values, y_cleveland.values,\n",
    "    features, targets_prior=expert_targets,\n",
    "    alphas=alphas, fractions=fractions\n",
    ")\n",
    "\n",
    "print(low_data_results.sort_values([\"train_frac\", \"model\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force_target_search(\n",
    "    X_train, y_train, X_test, y_test, features,\n",
    "    alphas=[0.5, 1.0, 2.0], multipliers=[0.5, 0.75, 1.0, 1.25, 1.5]\n",
    "):\n",
    "    \"\"\"\n",
    "    Brute force search: perturb baseline logistic regression coefficients\n",
    "    and test as priors for target-informed regression.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Step 1: Fit baseline logistic regression (MLE)\n",
    "    log_reg = LogisticRegression(penalty=None, max_iter=1000, solver=\"lbfgs\").fit(X_train, y_train)\n",
    "    base_coef = log_reg.coef_[0]\n",
    "\n",
    "    print(\"Baseline logistic coefficients (MLE):\", base_coef)\n",
    "\n",
    "    # Step 2: Generate perturbed targets\n",
    "    for alpha in alphas:\n",
    "        for m in multipliers:\n",
    "            mu = base_coef * m\n",
    "\n",
    "            # Step 3: Fit informed regression\n",
    "            informed = SklearnTargetModel(alpha=alpha, model_type=\"logistic\", targets=mu).fit(\n",
    "                X_train, y_train, feature_names=features\n",
    "            )\n",
    "            probs_inf = informed.predict(X_test)\n",
    "\n",
    "            # Step 4: Evaluate on Cleveland\n",
    "            metrics = {\n",
    "                \"alpha\": alpha,\n",
    "                \"multiplier\": m,\n",
    "                \"log_loss\": log_loss(y_test, probs_inf),\n",
    "                \"accuracy\": accuracy_score(y_test, (probs_inf > 0.5)),\n",
    "                \"auc\": roc_auc_score(y_test, probs_inf),\n",
    "            }\n",
    "            results.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline logistic coefficients (MLE): [-0.01179657  1.56139634 -0.00438747  0.00643127 -0.023387    1.62953566]\n",
      "    alpha  multiplier  log_loss  accuracy       auc\n",
      "20    5.0        0.50  0.568531  0.712871  0.806852\n",
      "21    5.0        0.75  0.575824  0.719472  0.806106\n",
      "16    2.0        0.75  0.584278  0.719472  0.806589\n",
      "15    2.0        0.50  0.584994  0.719472  0.805931\n",
      "11    1.0        0.75  0.587695  0.719472  0.806589\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "multipliers = [0.5, 0.75, 1.0, 1.25, 1.5]\n",
    "\n",
    "brute_results = brute_force_target_search(\n",
    "    X.values, y.values,\n",
    "    X_cleveland.values, y_cleveland.values,\n",
    "    features, alphas=alphas, multipliers=multipliers\n",
    ")\n",
    "\n",
    "print(brute_results.sort_values(\"log_loss\").head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score\n",
    "\n",
    "def brute_force_random_targets(\n",
    "    X_train, y_train, X_test, y_test, features,\n",
    "    alphas=[0.1, 0.5, 1.0, 2.0],\n",
    "    n_samples=1000000,  # up to \"millions\"\n",
    "    report_every=50000,\n",
    "    noise_scale=0.5,\n",
    "    random_state=42\n",
    "):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    results = []\n",
    "\n",
    "    # Step 1: Baseline MLE coefficients\n",
    "    log_reg = LogisticRegression(penalty=None, max_iter=1000, solver=\"lbfgs\").fit(X_train, y_train)\n",
    "    base_coef = log_reg.coef_[0]\n",
    "    print(\"Baseline logistic coefficients (MLE):\", base_coef)\n",
    "\n",
    "    best_logloss = float(\"inf\")\n",
    "    best_combo = None\n",
    "\n",
    "    # Step 2: Random sampling of target priors\n",
    "    for i in range(1, n_samples + 1):\n",
    "        # Random perturbation around baseline\n",
    "        noise = rng.normal(0, noise_scale, size=base_coef.shape)\n",
    "        mu = base_coef + noise\n",
    "\n",
    "        for alpha in alphas:\n",
    "            informed = SklearnTargetModel(alpha=alpha, model_type=\"logistic\", targets=mu).fit(\n",
    "                X_train, y_train, feature_names=features\n",
    "            )\n",
    "            probs_inf = informed.predict(X_test)\n",
    "\n",
    "            ll = log_loss(y_test, probs_inf)\n",
    "            auc = roc_auc_score(y_test, probs_inf)\n",
    "\n",
    "            if ll < best_logloss:\n",
    "                best_logloss = ll\n",
    "                best_combo = {\n",
    "                    \"iter\": i,\n",
    "                    \"alpha\": alpha,\n",
    "                    \"log_loss\": ll,\n",
    "                    \"auc\": auc,\n",
    "                    \"accuracy\": accuracy_score(y_test, (probs_inf > 0.5)),\n",
    "                    \"targets\": mu.round(3).tolist()\n",
    "                }\n",
    "\n",
    "            if i % report_every == 0 and alpha == alphas[0]:\n",
    "                print(f\"[Iteration {i}] Best so far: log_loss={best_logloss:.4f}, \"\n",
    "                      f\"AUC={best_combo['auc']:.3f}, alpha={best_combo['alpha']}, \"\n",
    "                      f\"targets={best_combo['targets']}\")\n",
    "\n",
    "    print(\"\\n=== Final Best Combo ===\")\n",
    "    print(best_combo)\n",
    "    return best_combo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline logistic coefficients (MLE): [-0.01179657  1.56139634 -0.00438747  0.00643127 -0.023387    1.62953566]\n",
      "[Iteration 20000] Best so far: log_loss=0.5572, AUC=0.808, alpha=0.1, targets=[0.091, 1.16, -0.069, 0.076, 0.08, 0.939]\n",
      "[Iteration 40000] Best so far: log_loss=0.5544, AUC=0.813, alpha=2.0, targets=[0.016, 1.33, 0.046, -0.029, -0.073, 0.919]\n",
      "[Iteration 60000] Best so far: log_loss=0.5544, AUC=0.813, alpha=2.0, targets=[0.016, 1.33, 0.046, -0.029, -0.073, 0.919]\n",
      "[Iteration 80000] Best so far: log_loss=0.5544, AUC=0.813, alpha=2.0, targets=[0.016, 1.33, 0.046, -0.029, -0.073, 0.919]\n",
      "[Iteration 100000] Best so far: log_loss=0.5544, AUC=0.813, alpha=2.0, targets=[0.016, 1.33, 0.046, -0.029, -0.073, 0.919]\n",
      "[Iteration 120000] Best so far: log_loss=0.5526, AUC=0.811, alpha=2.0, targets=[0.078, 1.01, 0.037, -0.025, -0.152, 1.05]\n",
      "[Iteration 140000] Best so far: log_loss=0.5526, AUC=0.811, alpha=2.0, targets=[0.078, 1.01, 0.037, -0.025, -0.152, 1.05]\n",
      "[Iteration 160000] Best so far: log_loss=0.5526, AUC=0.811, alpha=2.0, targets=[0.078, 1.01, 0.037, -0.025, -0.152, 1.05]\n",
      "[Iteration 180000] Best so far: log_loss=0.5515, AUC=0.822, alpha=1.0, targets=[0.109, 1.373, 0.161, -0.022, -0.2, 0.676]\n",
      "[Iteration 200000] Best so far: log_loss=0.5515, AUC=0.822, alpha=1.0, targets=[0.109, 1.373, 0.161, -0.022, -0.2, 0.676]\n",
      "\n",
      "=== Final Best Combo ===\n",
      "{'iter': 179881, 'alpha': 1.0, 'log_loss': 0.5515260715485178, 'auc': 0.822030180733462, 'accuracy': 0.7458745874587459, 'targets': [0.109, 1.373, 0.161, -0.022, -0.2, 0.676]}\n"
     ]
    }
   ],
   "source": [
    "best_prior = brute_force_random_targets(\n",
    "    X.values, y.values,\n",
    "    X_cleveland.values, y_cleveland.values,\n",
    "    features,\n",
    "    alphas=[0.1, 0.5, 1.0, 2.0],\n",
    "    n_samples=200000,    # adjust upward if you want millions\n",
    "    report_every=20000,  # progress updates\n",
    "    noise_scale=0.3      # controls how far we wander from MLE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}