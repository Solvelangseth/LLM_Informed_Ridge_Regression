{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Add src folder for custom models\n",
    "# Removed old path hack; using package imports))\n",
    "from llm_prior_project.models.target_model import SklearnTargetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Master dataset ready: (63504, 35)\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# 1. Data Loading\n",
    "# ==================================================\n",
    "def load_all_data(base_dir=\"mimic_data\"):\n",
    "    all_data = {}\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            all_data[folder] = {}\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    df_name = os.path.splitext(file)[0]\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, sep=None, engine=\"python\")\n",
    "                        all_data[folder][df_name] = df\n",
    "                    except Exception as e:\n",
    "                        print(f\"\u274c Could not read {file}: {e}\")\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def load_df(df, drop_index=True):\n",
    "    \"\"\"Drop default index columns if present.\"\"\"\n",
    "    if drop_index and \"Unnamed: 0\" in df.columns:\n",
    "        return df.drop(columns=[\"Unnamed: 0\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_master_dataset(all_data):\n",
    "    \"\"\"Merge demographics, ICU, lab, and physio data into one master table.\"\"\"\n",
    "    demo = load_df(all_data[\"DEMOGRAPHIC_DATA\"][\"DEMO_DATA\"])\n",
    "    icu_los = load_df(all_data[\"ICU_DATA\"][\"ICU_LOS\"])\n",
    "    total_los = load_df(all_data[\"ICU_DATA\"][\"TOTAL_LOS\"])\n",
    "    mech_vent = load_df(all_data[\"ICU_DATA\"][\"MECH_VENT_TIME\"])\n",
    "    severity = load_df(all_data[\"ICU_DATA\"][\"SEVERITY_SCORES\"])\n",
    "    services = all_data[\"ICU_DATA\"][\"SERVICES\"]\n",
    "    surgery = load_df(all_data[\"ICU_DATA\"][\"SURGERY_FLAGS\"])\n",
    "    icd9 = load_df(all_data[\"ICU_DATA\"][\"ICD9_DIAG\"])\n",
    "    prev_adm = load_df(all_data[\"ICU_DATA\"][\"PREVIOUS_ADMISSION_COUNT\"])\n",
    "\n",
    "    # Merge labs\n",
    "    labs = []\n",
    "    for df in all_data[\"LAB_DATA\"].values():\n",
    "        df = load_df(df)\n",
    "        avg_cols = [c for c in df.columns if c.startswith(\"avg_\")]\n",
    "        labs.append(df[[\"hadm_id\"] + avg_cols].copy())\n",
    "    lab_df = pd.concat(labs, axis=1).loc[:, ~pd.concat(labs, axis=1).columns.duplicated()]\n",
    "\n",
    "    # Merge physio\n",
    "    physio = []\n",
    "    for df in all_data[\"PHYSIO_DATA\"].values():\n",
    "        df = load_df(df)\n",
    "        avg_cols = [c for c in df.columns if c.startswith(\"avg_\")]\n",
    "        physio.append(df[[\"hadm_id\"] + avg_cols].copy())\n",
    "    physio_df = pd.concat(physio, axis=1).loc[:, ~pd.concat(physio, axis=1).columns.duplicated()]\n",
    "\n",
    "    master = (\n",
    "        demo.merge(icu_los, on=\"hadm_id\", how=\"left\")\n",
    "        .merge(total_los, on=\"hadm_id\", how=\"left\")\n",
    "        .merge(mech_vent, on=\"hadm_id\", how=\"left\")\n",
    "        .merge(severity, on=\"hadm_id\", how=\"left\")\n",
    "        .merge(services, on=\"hadm_id\", how=\"left\")\n",
    "        .merge(surgery, on=\"hadm_id\", how=\"left\")\n",
    "        .merge(icd9, on=\"hadm_id\", how=\"left\")\n",
    "        .merge(prev_adm, on=\"hadm_id\", how=\"left\")\n",
    "        .merge(lab_df, on=\"hadm_id\", how=\"left\")\n",
    "        .merge(physio_df, on=\"hadm_id\", how=\"left\")\n",
    "    )\n",
    "    return master\n",
    "\n",
    "\n",
    "# Load data\n",
    "all_data = load_all_data()\n",
    "master = build_master_dataset(all_data)\n",
    "master_clean = master.dropna(subset=[\"los\"])\n",
    "\n",
    "print(\"\u2705 Master dataset ready:\", master_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ==================================================\n",
    "# Preprocessing Setup\n",
    "# ==================================================\n",
    "def build_preprocessor(X, target=\"los\", exclude_feats=None):\n",
    "    \"\"\"\n",
    "    Build preprocessing pipeline for numeric + categorical features,\n",
    "    while excluding leakage features like LOS itself.\n",
    "    \"\"\"\n",
    "    if exclude_feats is None:\n",
    "        exclude_feats = []\n",
    "\n",
    "    # make sure target is always excluded\n",
    "    exclude_feats = set(exclude_feats + [target])\n",
    "\n",
    "    categorical_features = [\n",
    "        c for c in X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        if c not in exclude_feats\n",
    "    ]\n",
    "    numeric_features = [\n",
    "        c for c in X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "        if c not in exclude_feats\n",
    "    ]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor, numeric_features, categorical_features\n",
    "\n",
    "\n",
    "preprocessor, num_feats, cat_feats = build_preprocessor(master_clean, target=\"los\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 35\n",
      "['hadm_id', 'age', 'gender', 'marital_status', 'religion', 'ethnicity', 'los', 'total_los_days', 'total_mech_vent_time', 'oasis_avg', 'sofa_avg', 'saps_avg', 'service', 'SURGERY_FLAG', 'icd9_group', 'admissions_count', 'subject_id', 'avg_creatinine', 'avg_white_blood_cells', 'avg_blood_glucose', 'avg_bicarbonate', 'avg_platelet_count', 'avg_hematrocrit', 'avg_albumin', 'avg_potasssium', 'avg_sodium', 'avg_blood_urea_nitrogen', 'avg_sys_press', 'avg_temp', 'avg_resp_rate', 'avg_hr', 'avg_cvp', 'avg_spo2', 'avg_art_ph', 'avg_dias_press']\n",
      "Top missingness:\n",
      "avg_temp                   0.895156\n",
      "avg_art_ph                 0.683122\n",
      "avg_cvp                    0.676776\n",
      "total_mech_vent_time       0.541053\n",
      "avg_albumin                0.466427\n",
      "avg_spo2                   0.141802\n",
      "avg_dias_press             0.141172\n",
      "avg_sys_press              0.141172\n",
      "avg_blood_glucose          0.128086\n",
      "SURGERY_FLAG               0.126968\n",
      "avg_bicarbonate            0.122229\n",
      "avg_potasssium             0.122166\n",
      "avg_platelet_count         0.122166\n",
      "avg_creatinine             0.122166\n",
      "avg_white_blood_cells      0.122150\n",
      "avg_hematrocrit            0.122134\n",
      "avg_sodium                 0.122134\n",
      "avg_blood_urea_nitrogen    0.122134\n",
      "avg_resp_rate              0.018660\n",
      "avg_hr                     0.017936\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>religion</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>los</th>\n",
       "      <th>total_los_days</th>\n",
       "      <th>total_mech_vent_time</th>\n",
       "      <th>oasis_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_sodium</th>\n",
       "      <th>avg_blood_urea_nitrogen</th>\n",
       "      <th>avg_sys_press</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>avg_resp_rate</th>\n",
       "      <th>avg_hr</th>\n",
       "      <th>avg_cvp</th>\n",
       "      <th>avg_spo2</th>\n",
       "      <th>avg_art_ph</th>\n",
       "      <th>avg_dias_press</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165315</td>\n",
       "      <td>64.971282</td>\n",
       "      <td>F</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>NONE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1.1438</td>\n",
       "      <td>1.144444</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>135.263158</td>\n",
       "      <td>12.230769</td>\n",
       "      <td>141.538462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.912500</td>\n",
       "      <td>70.609375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.779221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.486111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152223</td>\n",
       "      <td>71.178910</td>\n",
       "      <td>M</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>CHRISTIAN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1.2641</td>\n",
       "      <td>5.496528</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>137.428571</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>101.340909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.101695</td>\n",
       "      <td>94.435103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124321</td>\n",
       "      <td>75.306343</td>\n",
       "      <td>M</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>CHRISTIAN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1.1862</td>\n",
       "      <td>6.768056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>140.571429</td>\n",
       "      <td>65.200000</td>\n",
       "      <td>122.180000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.130435</td>\n",
       "      <td>150.500000</td>\n",
       "      <td>5.266667</td>\n",
       "      <td>97.781022</td>\n",
       "      <td>7.413333</td>\n",
       "      <td>65.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161859</td>\n",
       "      <td>39.042949</td>\n",
       "      <td>M</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>CHRISTIAN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>2.856944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>139.200000</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>122.937500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.890728</td>\n",
       "      <td>95.347826</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.889273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>129635</td>\n",
       "      <td>58.989281</td>\n",
       "      <td>M</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>NONE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>3.5466</td>\n",
       "      <td>3.534028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>138.466667</td>\n",
       "      <td>11.653846</td>\n",
       "      <td>115.065934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.595238</td>\n",
       "      <td>137.259804</td>\n",
       "      <td>10.344828</td>\n",
       "      <td>95.238095</td>\n",
       "      <td>7.527692</td>\n",
       "      <td>59.520000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows \u00d7 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hadm_id        age gender marital_status   religion ethnicity     los  \\\n",
       "0   165315  64.971282      F        MARRIED       NONE     WHITE  1.1438   \n",
       "1   152223  71.178910      M        MARRIED  CHRISTIAN     WHITE  1.2641   \n",
       "2   124321  75.306343      M        MARRIED  CHRISTIAN     WHITE  1.1862   \n",
       "3   161859  39.042949      M         SINGLE  CHRISTIAN     WHITE  0.5124   \n",
       "4   129635  58.989281      M        MARRIED       NONE     WHITE  3.5466   \n",
       "\n",
       "   total_los_days  total_mech_vent_time  oasis_avg  ...  avg_sodium  \\\n",
       "0        1.144444              5.000000       41.0  ...  135.263158   \n",
       "1        5.496528              4.666667       24.0  ...  137.428571   \n",
       "2        6.768056                   NaN       24.0  ...  140.571429   \n",
       "3        2.856944                   NaN       15.0  ...  139.200000   \n",
       "4        3.534028                   NaN       24.0  ...  138.466667   \n",
       "\n",
       "   avg_blood_urea_nitrogen avg_sys_press avg_temp avg_resp_rate      avg_hr  \\\n",
       "0                12.230769    141.538462      NaN     19.912500   70.609375   \n",
       "1                 9.000000    101.340909      NaN     22.101695   94.435103   \n",
       "2                65.200000    122.180000      NaN     17.130435  150.500000   \n",
       "3                14.750000    122.937500      NaN     25.890728   95.347826   \n",
       "4                11.653846    115.065934      NaN     17.595238  137.259804   \n",
       "\n",
       "     avg_cvp   avg_spo2  avg_art_ph  avg_dias_press  \n",
       "0        NaN  97.779221         NaN       53.486111  \n",
       "1        NaN  96.900000         NaN       52.500000  \n",
       "2   5.266667  97.781022    7.413333       65.424000  \n",
       "3        NaN  98.889273         NaN       43.150000  \n",
       "4  10.344828  95.238095    7.527692       59.520000  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# ==================================================\n",
    "# Feature exclusions (junk / leakage)\n",
    "# ==================================================\n",
    "exclude_feats = [\"hadm_id\", \"subject_id\"]\n",
    "\n",
    "# Preview dataset structure\n",
    "print(\"Columns:\", len(master_clean.columns))\n",
    "print(master_clean.columns.tolist()[:40])  # peek first 40 columns\n",
    "\n",
    "# Look at missingness\n",
    "missing_summary = master_clean.isna().mean().sort_values(ascending=False)\n",
    "print(\"Top missingness:\")\n",
    "print(missing_summary.head(20))\n",
    "\n",
    "# Preview a few rows\n",
    "master_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethnicity\n",
      "WHITE       44713\n",
      "NONE         6441\n",
      "BLACK        6120\n",
      "HISPANIC     2265\n",
      "ASIAN        2075\n",
      "OTHER        1890\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WHITE</th>\n",
       "      <td>44713.0</td>\n",
       "      <td>4.727738</td>\n",
       "      <td>9.002312</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.120200</td>\n",
       "      <td>2.08970</td>\n",
       "      <td>4.381300</td>\n",
       "      <td>173.0725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONE</th>\n",
       "      <td>6441.0</td>\n",
       "      <td>5.525501</td>\n",
       "      <td>10.206800</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>1.204000</td>\n",
       "      <td>2.31640</td>\n",
       "      <td>5.238600</td>\n",
       "      <td>169.4202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLACK</th>\n",
       "      <td>6120.0</td>\n",
       "      <td>4.999474</td>\n",
       "      <td>10.913450</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>1.070775</td>\n",
       "      <td>2.03735</td>\n",
       "      <td>4.084800</td>\n",
       "      <td>171.6227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HISPANIC</th>\n",
       "      <td>2265.0</td>\n",
       "      <td>4.743731</td>\n",
       "      <td>9.537550</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>1.047800</td>\n",
       "      <td>1.93130</td>\n",
       "      <td>4.140000</td>\n",
       "      <td>133.2542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASIAN</th>\n",
       "      <td>2075.0</td>\n",
       "      <td>4.739803</td>\n",
       "      <td>11.397314</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>1.57390</td>\n",
       "      <td>3.687250</td>\n",
       "      <td>126.8261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTHER</th>\n",
       "      <td>1890.0</td>\n",
       "      <td>5.636142</td>\n",
       "      <td>11.584511</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>1.047850</td>\n",
       "      <td>1.94985</td>\n",
       "      <td>4.878875</td>\n",
       "      <td>135.5667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count      mean        std     min       25%      50%       75%  \\\n",
       "ethnicity                                                                      \n",
       "WHITE      44713.0  4.727738   9.002312  0.0001  1.120200  2.08970  4.381300   \n",
       "NONE        6441.0  5.525501  10.206800  0.0014  1.204000  2.31640  5.238600   \n",
       "BLACK       6120.0  4.999474  10.913450  0.0025  1.070775  2.03735  4.084800   \n",
       "HISPANIC    2265.0  4.743731   9.537550  0.0019  1.047800  1.93130  4.140000   \n",
       "ASIAN       2075.0  4.739803  11.397314  0.0010  0.418800  1.57390  3.687250   \n",
       "OTHER       1890.0  5.636142  11.584511  0.0209  1.047850  1.94985  4.878875   \n",
       "\n",
       "                max  \n",
       "ethnicity            \n",
       "WHITE      173.0725  \n",
       "NONE       169.4202  \n",
       "BLACK      171.6227  \n",
       "HISPANIC   133.2542  \n",
       "ASIAN      126.8261  \n",
       "OTHER      135.5667  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ethnicity</th>\n",
       "      <th>ASIAN</th>\n",
       "      <th>BLACK</th>\n",
       "      <th>HISPANIC</th>\n",
       "      <th>NONE</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>WHITE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>898</td>\n",
       "      <td>3374</td>\n",
       "      <td>894</td>\n",
       "      <td>2636</td>\n",
       "      <td>770</td>\n",
       "      <td>19282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>1177</td>\n",
       "      <td>2746</td>\n",
       "      <td>1371</td>\n",
       "      <td>3805</td>\n",
       "      <td>1120</td>\n",
       "      <td>25431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ethnicity  ASIAN  BLACK  HISPANIC  NONE  OTHER  WHITE\n",
       "gender                                               \n",
       "F            898   3374       894  2636    770  19282\n",
       "M           1177   2746      1371  3805   1120  25431"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# ==================================================\n",
    "# EDA: subgroup counts and LOS distribution\n",
    "# ==================================================\n",
    "# Ethnicity distribution\n",
    "print(master_clean[\"ethnicity\"].value_counts(dropna=False).head(20))\n",
    "\n",
    "# LOS summary by ethnicity\n",
    "los_summary = (\n",
    "    master_clean.groupby(\"ethnicity\")[\"los\"]\n",
    "    .describe(percentiles=[0.25,0.5,0.75])\n",
    "    .sort_values(\"count\", ascending=False)\n",
    ")\n",
    "display(los_summary.head(10))\n",
    "\n",
    "# Gender \u00d7 Ethnicity crosstab\n",
    "pd.crosstab(master_clean[\"gender\"], master_clean[\"ethnicity\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: 25\n",
      "Categorical features: 7\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ==================================================\n",
    "# Preprocessor with exclusions\n",
    "# ==================================================\n",
    "preprocessor, num_feats, cat_feats = build_preprocessor(\n",
    "    master_clean, target=\"los\", exclude_feats=exclude_feats\n",
    ")\n",
    "\n",
    "print(\"Numeric features:\", len(num_feats))\n",
    "print(\"Categorical features:\", len(cat_feats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ==================================================\n",
    "# Subgroup Opportunity Sweep\n",
    "# ==================================================\n",
    "from collections import defaultdict\n",
    "\n",
    "def subgroup_opportunity(X, y, groups, alpha=1.0, min_n=200):\n",
    "    \"\"\"\n",
    "    Compare coefficients between subgroup-specific Ridge vs complement Ridge.\n",
    "    Compute opportunity index for each feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        Encoded feature matrix\n",
    "    y : Series or ndarray\n",
    "        Outcome vector\n",
    "    groups : Series\n",
    "        Subgroup labels\n",
    "    alpha : float\n",
    "        Ridge regularization strength\n",
    "    min_n : int\n",
    "        Minimum subgroup size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    per_feature_df : pd.DataFrame\n",
    "        Feature-level opportunity scores for this grouping\n",
    "    summary : pd.DataFrame\n",
    "        Group-level summary (max opp per subgroup)\n",
    "    \"\"\"\n",
    "    feature_names = X.columns if isinstance(X, pd.DataFrame) else np.arange(X.shape[1])\n",
    "    results = []\n",
    "    per_feature = {}\n",
    "\n",
    "    for gval, idx in groups.groupby(groups):\n",
    "        if len(idx) < min_n or gval is None or gval != gval:  # skip NaN/low n\n",
    "            continue\n",
    "\n",
    "        mask = groups == gval\n",
    "        X_sub, y_sub = X[mask], y[mask]\n",
    "        X_comp, y_comp = X[~mask], y[~mask]\n",
    "\n",
    "        # Ridge on subgroup\n",
    "        ridge_sub = Ridge(alpha=alpha).fit(X_sub, y_sub)\n",
    "        coef_sub = ridge_sub.coef_\n",
    "\n",
    "        # Ridge on complement\n",
    "        ridge_comp = Ridge(alpha=alpha).fit(X_comp, y_comp)\n",
    "        coef_comp = ridge_comp.coef_\n",
    "\n",
    "        # Opportunity: delta * std\n",
    "        delta = coef_sub - coef_comp\n",
    "        std_sub = X_sub.std(axis=0)\n",
    "        opp = np.abs(delta) * std_sub\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"feature\": feature_names,\n",
    "            \"delta_beta\": delta,\n",
    "            \"std_subgroup\": std_sub,\n",
    "            \"opportunity_index\": opp\n",
    "        }).sort_values(\"opportunity_index\", ascending=False)\n",
    "\n",
    "        per_feature[(groups.name, gval)] = df\n",
    "        results.append({\n",
    "            \"group\": groups.name,\n",
    "            \"value\": gval,\n",
    "            \"n\": len(X_sub),\n",
    "            \"max_opportunity\": df[\"opportunity_index\"].max(),\n",
    "            \"top_feature\": df.iloc[0][\"feature\"]\n",
    "        })\n",
    "\n",
    "    summary = pd.DataFrame(results).sort_values(\"max_opportunity\", ascending=False)\n",
    "    return per_feature, summary\n",
    "\n",
    "\n",
    "def run_subgroup_sweep(\n",
    "    df, subgroup_specs, exclude_feats=None, alpha_grid=[1.0], min_group_n=200\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper: run subgroup_opportunity across multiple subgroup specs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Clean dataset including target + subgroup columns\n",
    "    subgroup_specs : list of (col, val) or (col, None)\n",
    "        Subgroups to evaluate\n",
    "    exclude_feats : list\n",
    "        Columns to exclude from predictors\n",
    "    alpha_grid : list\n",
    "        Alphas to test (report best per group)\n",
    "    min_group_n : int\n",
    "        Minimum subgroup size\n",
    "    \"\"\"\n",
    "    if exclude_feats is None:\n",
    "        exclude_feats = []\n",
    "\n",
    "    results = []\n",
    "    per_feature_all = {}\n",
    "\n",
    "    # Build X, y once\n",
    "    y = df[\"los\"]\n",
    "    X = df.drop(columns=exclude_feats).drop(columns=[\"los\"])\n",
    "\n",
    "    # One-hot encode categoricals for coefficient comparability\n",
    "    X_enc = pd.get_dummies(X, drop_first=True)\n",
    "    feature_names = X_enc.columns\n",
    "\n",
    "    for col, val in subgroup_specs:\n",
    "        groups = df[col]\n",
    "\n",
    "        best_alpha, best_summary, best_feat = None, None, None\n",
    "        best_score = -np.inf\n",
    "\n",
    "        for alpha in alpha_grid:\n",
    "            per_feature, summary = subgroup_opportunity(\n",
    "                X_enc, y, groups, alpha=alpha, min_n=min_group_n\n",
    "            )\n",
    "            if not summary.empty:\n",
    "                # Use average max_opportunity as score\n",
    "                score = summary[\"max_opportunity\"].mean()\n",
    "                if score > best_score:\n",
    "                    best_alpha = alpha\n",
    "                    best_summary = summary\n",
    "                    best_feat = per_feature\n",
    "                    best_score = score\n",
    "\n",
    "        if best_summary is not None:\n",
    "            results.append(best_summary.assign(alpha=best_alpha))\n",
    "            per_feature_all.update(best_feat)\n",
    "\n",
    "    return pd.concat(results, ignore_index=True), per_feature_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# ==================================================\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Subgroup opportunity analysis (refined)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# ==================================================\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m subgroup_specs \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39methnicity\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mgender\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mservice\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m subgroup_report, per_feature_opp \u001b[39m=\u001b[39m run_subgroup_sweep(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     master_clean,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     subgroup_specs\u001b[39m=\u001b[39;49msubgroup_specs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     exclude_feats\u001b[39m=\u001b[39;49mexclude_feats,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     alpha_grid\u001b[39m=\u001b[39;49m[\u001b[39m0.1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m10\u001b[39;49m, \u001b[39m100\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     min_group_n\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m display(subgroup_report\u001b[39m.\u001b[39mhead(\u001b[39m15\u001b[39m))\n",
      "\u001b[1;32m/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m best_score \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mfor\u001b[39;00m alpha \u001b[39min\u001b[39;00m alpha_grid:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     per_feature, summary \u001b[39m=\u001b[39m subgroup_opportunity(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m         X_enc, y, groups, alpha\u001b[39m=\u001b[39;49malpha, min_n\u001b[39m=\u001b[39;49mmin_group_n\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m summary\u001b[39m.\u001b[39mempty:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m         \u001b[39m# Use average max_opportunity as score\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m         score \u001b[39m=\u001b[39m summary[\u001b[39m\"\u001b[39m\u001b[39mmax_opportunity\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmean()\n",
      "\u001b[1;32m/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m X_comp, y_comp \u001b[39m=\u001b[39m X[\u001b[39m~\u001b[39mmask], y[\u001b[39m~\u001b[39mmask]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Ridge on subgroup\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m ridge_sub \u001b[39m=\u001b[39m Ridge(alpha\u001b[39m=\u001b[39;49malpha)\u001b[39m.\u001b[39;49mfit(X_sub, y_sub)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m coef_sub \u001b[39m=\u001b[39m ridge_sub\u001b[39m.\u001b[39mcoef_\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solvelangseth/master/llm-statistical-learning/notebooks/mimic_2.ipynb#Y100sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Ridge on complement\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tir-env/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tir-env/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:1167\u001b[0m, in \u001b[0;36mRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit Ridge regression model.\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \n\u001b[1;32m   1149\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m _accept_sparse \u001b[39m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[39m.\u001b[39missparse(X), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver)\n\u001b[0;32m-> 1167\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1168\u001b[0m     X,\n\u001b[1;32m   1169\u001b[0m     y,\n\u001b[1;32m   1170\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m_accept_sparse,\n\u001b[1;32m   1171\u001b[0m     dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[1;32m   1172\u001b[0m     multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1173\u001b[0m     y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1174\u001b[0m )\n\u001b[1;32m   1175\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/miniconda3/envs/tir-env/lib/python3.10/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/tir-env/lib/python3.10/site-packages/sklearn/utils/validation.py:1263\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1259\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1260\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     )\n\u001b[0;32m-> 1263\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1264\u001b[0m     X,\n\u001b[1;32m   1265\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1266\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1267\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1268\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1269\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1270\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1271\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1272\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1273\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1274\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1275\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1276\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1277\u001b[0m )\n\u001b[1;32m   1279\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1281\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/tir-env/lib/python3.10/site-packages/sklearn/utils/validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1044\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1049\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1050\u001b[0m         array,\n\u001b[1;32m   1051\u001b[0m         input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m   1052\u001b[0m         estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m   1053\u001b[0m         allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[1;32m   1056\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m   1057\u001b[0m     \u001b[39mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1058\u001b[0m         \u001b[39m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tir-env/lib/python3.10/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    127\u001b[0m     X,\n\u001b[1;32m    128\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[1;32m    129\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[1;32m    130\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[1;32m    131\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    132\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    133\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tir-env/lib/python3.10/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ==================================================\n",
    "# Subgroup opportunity analysis (refined)\n",
    "# ==================================================\n",
    "subgroup_specs = [\n",
    "    (\"ethnicity\", None),\n",
    "    (\"gender\", None),\n",
    "    (\"service\", None),\n",
    "]\n",
    "\n",
    "subgroup_report, per_feature_opp = run_subgroup_sweep(\n",
    "    master_clean,\n",
    "    subgroup_specs=subgroup_specs,\n",
    "    exclude_feats=exclude_feats,\n",
    "    alpha_grid=[0.1, 1, 10, 100],\n",
    "    min_group_n=200\n",
    ")\n",
    "\n",
    "display(subgroup_report.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}